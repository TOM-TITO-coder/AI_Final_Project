{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.996823</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.996823</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.996802</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.158900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.996802</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.139886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.996781</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566501</th>\n",
       "      <td>-1.129466</td>\n",
       "      <td>-7.145238</td>\n",
       "      <td>5.383562</td>\n",
       "      <td>-9.679664</td>\n",
       "      <td>7.365853</td>\n",
       "      <td>-7.135087</td>\n",
       "      <td>-3.107715</td>\n",
       "      <td>-11.258176</td>\n",
       "      <td>4.501765</td>\n",
       "      <td>-6.438760</td>\n",
       "      <td>-12.633397</td>\n",
       "      <td>7.673385</td>\n",
       "      <td>-13.954257</td>\n",
       "      <td>0.645999</td>\n",
       "      <td>-15.253431</td>\n",
       "      <td>1.341926</td>\n",
       "      <td>-12.118952</td>\n",
       "      <td>-21.374216</td>\n",
       "      <td>-7.753470</td>\n",
       "      <td>3.539377</td>\n",
       "      <td>0.886165</td>\n",
       "      <td>2.215141</td>\n",
       "      <td>0.250707</td>\n",
       "      <td>0.035547</td>\n",
       "      <td>0.440521</td>\n",
       "      <td>0.187297</td>\n",
       "      <td>0.525468</td>\n",
       "      <td>1.502104</td>\n",
       "      <td>0.287044</td>\n",
       "      <td>-0.171154</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566502</th>\n",
       "      <td>-1.702343</td>\n",
       "      <td>-4.655386</td>\n",
       "      <td>8.532752</td>\n",
       "      <td>-15.222321</td>\n",
       "      <td>10.198547</td>\n",
       "      <td>-4.197514</td>\n",
       "      <td>-3.299981</td>\n",
       "      <td>-10.610038</td>\n",
       "      <td>4.919960</td>\n",
       "      <td>-5.728580</td>\n",
       "      <td>-11.811876</td>\n",
       "      <td>11.183936</td>\n",
       "      <td>-16.630439</td>\n",
       "      <td>0.258371</td>\n",
       "      <td>-17.566520</td>\n",
       "      <td>-0.470995</td>\n",
       "      <td>-10.209341</td>\n",
       "      <td>-13.895245</td>\n",
       "      <td>-5.029679</td>\n",
       "      <td>1.050051</td>\n",
       "      <td>1.430766</td>\n",
       "      <td>1.974833</td>\n",
       "      <td>0.195956</td>\n",
       "      <td>0.482864</td>\n",
       "      <td>-1.203050</td>\n",
       "      <td>-0.340363</td>\n",
       "      <td>0.639373</td>\n",
       "      <td>1.624504</td>\n",
       "      <td>0.731982</td>\n",
       "      <td>-0.349333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566503</th>\n",
       "      <td>0.748914</td>\n",
       "      <td>1.187708</td>\n",
       "      <td>3.089764</td>\n",
       "      <td>-6.134725</td>\n",
       "      <td>5.575777</td>\n",
       "      <td>1.604699</td>\n",
       "      <td>-1.933068</td>\n",
       "      <td>-0.965297</td>\n",
       "      <td>0.292589</td>\n",
       "      <td>-2.962763</td>\n",
       "      <td>-3.891017</td>\n",
       "      <td>4.312109</td>\n",
       "      <td>-4.802467</td>\n",
       "      <td>-1.459676</td>\n",
       "      <td>-10.097988</td>\n",
       "      <td>-0.350808</td>\n",
       "      <td>0.508201</td>\n",
       "      <td>0.237821</td>\n",
       "      <td>1.503925</td>\n",
       "      <td>-1.873095</td>\n",
       "      <td>0.267961</td>\n",
       "      <td>0.107235</td>\n",
       "      <td>-0.915751</td>\n",
       "      <td>-0.058251</td>\n",
       "      <td>0.069976</td>\n",
       "      <td>0.353141</td>\n",
       "      <td>0.063025</td>\n",
       "      <td>0.479362</td>\n",
       "      <td>0.326816</td>\n",
       "      <td>-0.353327</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566504</th>\n",
       "      <td>-1.718906</td>\n",
       "      <td>-1.630195</td>\n",
       "      <td>3.201962</td>\n",
       "      <td>-4.189262</td>\n",
       "      <td>2.308225</td>\n",
       "      <td>-1.419808</td>\n",
       "      <td>-1.820375</td>\n",
       "      <td>-3.521141</td>\n",
       "      <td>1.537790</td>\n",
       "      <td>-1.185527</td>\n",
       "      <td>-6.203047</td>\n",
       "      <td>5.350961</td>\n",
       "      <td>-7.327920</td>\n",
       "      <td>1.352953</td>\n",
       "      <td>-6.463558</td>\n",
       "      <td>-0.544129</td>\n",
       "      <td>-2.652545</td>\n",
       "      <td>-4.963069</td>\n",
       "      <td>-0.615102</td>\n",
       "      <td>0.374866</td>\n",
       "      <td>0.292891</td>\n",
       "      <td>0.516993</td>\n",
       "      <td>-0.499770</td>\n",
       "      <td>-0.060555</td>\n",
       "      <td>-0.418137</td>\n",
       "      <td>0.100704</td>\n",
       "      <td>0.330817</td>\n",
       "      <td>0.268986</td>\n",
       "      <td>0.119987</td>\n",
       "      <td>-0.332985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566505</th>\n",
       "      <td>-0.671024</td>\n",
       "      <td>-4.455152</td>\n",
       "      <td>1.107300</td>\n",
       "      <td>-5.120020</td>\n",
       "      <td>4.112380</td>\n",
       "      <td>-1.542035</td>\n",
       "      <td>-2.480118</td>\n",
       "      <td>-7.057722</td>\n",
       "      <td>2.003581</td>\n",
       "      <td>-3.098341</td>\n",
       "      <td>-7.909891</td>\n",
       "      <td>5.865746</td>\n",
       "      <td>-8.812094</td>\n",
       "      <td>-1.027205</td>\n",
       "      <td>-10.478193</td>\n",
       "      <td>-0.790313</td>\n",
       "      <td>-7.916520</td>\n",
       "      <td>-14.060588</td>\n",
       "      <td>-4.791543</td>\n",
       "      <td>2.034585</td>\n",
       "      <td>0.350113</td>\n",
       "      <td>1.144887</td>\n",
       "      <td>0.113788</td>\n",
       "      <td>-0.229210</td>\n",
       "      <td>-0.329808</td>\n",
       "      <td>0.053415</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>1.680269</td>\n",
       "      <td>0.205392</td>\n",
       "      <td>-0.327712</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>566506 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2  ...       V28    Amount  Class\n",
       "0      -1.996823 -1.359807 -0.072781  ... -0.021053  0.244200      0\n",
       "1      -1.996823  1.191857  0.266151  ...  0.014724 -0.342584      0\n",
       "2      -1.996802 -1.358354 -1.340163  ... -0.059752  1.158900      0\n",
       "3      -1.996802 -0.966272 -0.185226  ...  0.061458  0.139886      0\n",
       "4      -1.996781 -1.158233  0.877737  ...  0.215153 -0.073813      0\n",
       "...          ...       ...       ...  ...       ...       ...    ...\n",
       "566501 -1.129466 -7.145238  5.383562  ...  0.287044 -0.171154      1\n",
       "566502 -1.702343 -4.655386  8.532752  ...  0.731982 -0.349333      1\n",
       "566503  0.748914  1.187708  3.089764  ...  0.326816 -0.353327      1\n",
       "566504 -1.718906 -1.630195  3.201962  ...  0.119987 -0.332985      1\n",
       "566505 -0.671024 -4.455152  1.107300  ...  0.205392 -0.327712      1\n",
       "\n",
       "[566506 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('../data/creditcard.csv')\n",
    "data = pd.read_csv('../data/oversampling_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "X = data.drop('Class', axis=1)\n",
    "y = data['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([453204, 30]), torch.Size([453204]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting to tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "X_train_tensor.shape, y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)  # First hidden layer\n",
    "        self.fc2 = nn.Linear(64, 32)         # Second hidden layer\n",
    "        self.fc3 = nn.Linear(32, 1)          # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()          # Sigmoid activation function\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))          # ReLU activation function\n",
    "        x = torch.relu(self.fc2(x))          # ReLU activation function\n",
    "        x = self.sigmoid(self.fc3(x))        # Sigmoid activation function for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(X_train_tensor.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.15875987708568573\n",
      "Epoch 2/100, Loss: 0.14639078080654144\n",
      "Epoch 3/100, Loss: 0.13575419783592224\n",
      "Epoch 4/100, Loss: 0.12843254208564758\n",
      "Epoch 5/100, Loss: 0.123184435069561\n",
      "Epoch 6/100, Loss: 0.11907161772251129\n",
      "Epoch 7/100, Loss: 0.11599080264568329\n",
      "Epoch 8/100, Loss: 0.11740411072969437\n",
      "Epoch 9/100, Loss: 0.11352124065160751\n",
      "Epoch 10/100, Loss: 0.11413726955652237\n",
      "Epoch 11/100, Loss: 0.11253713816404343\n",
      "Epoch 12/100, Loss: 0.10957285016775131\n",
      "Epoch 13/100, Loss: 0.10607617348432541\n",
      "Epoch 14/100, Loss: 0.10257934778928757\n",
      "Epoch 15/100, Loss: 0.09953141957521439\n",
      "Epoch 16/100, Loss: 0.09668033570051193\n",
      "Epoch 17/100, Loss: 0.09356603026390076\n",
      "Epoch 18/100, Loss: 0.0900459885597229\n",
      "Epoch 19/100, Loss: 0.08678417652845383\n",
      "Epoch 20/100, Loss: 0.08426533639431\n",
      "Epoch 21/100, Loss: 0.0821559801697731\n",
      "Epoch 22/100, Loss: 0.0800512358546257\n",
      "Epoch 23/100, Loss: 0.07775142043828964\n",
      "Epoch 24/100, Loss: 0.07530784606933594\n",
      "Epoch 25/100, Loss: 0.07313386350870132\n",
      "Epoch 26/100, Loss: 0.07151658087968826\n",
      "Epoch 27/100, Loss: 0.07057163864374161\n",
      "Epoch 28/100, Loss: 0.068968765437603\n",
      "Epoch 29/100, Loss: 0.06726943701505661\n",
      "Epoch 30/100, Loss: 0.06580844521522522\n",
      "Epoch 31/100, Loss: 0.06450196355581284\n",
      "Epoch 32/100, Loss: 0.06319955736398697\n",
      "Epoch 33/100, Loss: 0.06188060715794563\n",
      "Epoch 34/100, Loss: 0.06033322215080261\n",
      "Epoch 35/100, Loss: 0.059000272303819656\n",
      "Epoch 36/100, Loss: 0.057446617633104324\n",
      "Epoch 37/100, Loss: 0.05596541613340378\n",
      "Epoch 38/100, Loss: 0.054378390312194824\n",
      "Epoch 39/100, Loss: 0.05305439978837967\n",
      "Epoch 40/100, Loss: 0.0518534816801548\n",
      "Epoch 41/100, Loss: 0.05068501830101013\n",
      "Epoch 42/100, Loss: 0.049909986555576324\n",
      "Epoch 43/100, Loss: 0.048742905259132385\n",
      "Epoch 44/100, Loss: 0.048486050218343735\n",
      "Epoch 45/100, Loss: 0.04793684184551239\n",
      "Epoch 46/100, Loss: 0.047563668340444565\n",
      "Epoch 47/100, Loss: 0.04658282548189163\n",
      "Epoch 48/100, Loss: 0.04563123732805252\n",
      "Epoch 49/100, Loss: 0.04477065056562424\n",
      "Epoch 50/100, Loss: 0.04410293325781822\n",
      "Epoch 51/100, Loss: 0.04320402070879936\n",
      "Epoch 52/100, Loss: 0.04232850298285484\n",
      "Epoch 53/100, Loss: 0.041510943323373795\n",
      "Epoch 54/100, Loss: 0.040684737265110016\n",
      "Epoch 55/100, Loss: 0.03985101357102394\n",
      "Epoch 56/100, Loss: 0.03907112404704094\n",
      "Epoch 57/100, Loss: 0.03833020478487015\n",
      "Epoch 58/100, Loss: 0.03758715093135834\n",
      "Epoch 59/100, Loss: 0.03686597943305969\n",
      "Epoch 60/100, Loss: 0.03618356212973595\n",
      "Epoch 61/100, Loss: 0.03550879657268524\n",
      "Epoch 62/100, Loss: 0.03485254570841789\n",
      "Epoch 63/100, Loss: 0.034242574125528336\n",
      "Epoch 64/100, Loss: 0.03365382179617882\n",
      "Epoch 65/100, Loss: 0.03325457125902176\n",
      "Epoch 66/100, Loss: 0.0326995812356472\n",
      "Epoch 67/100, Loss: 0.03216296061873436\n",
      "Epoch 68/100, Loss: 0.03162665665149689\n",
      "Epoch 69/100, Loss: 0.031118057668209076\n",
      "Epoch 70/100, Loss: 0.03063325211405754\n",
      "Epoch 71/100, Loss: 0.03016064502298832\n",
      "Epoch 72/100, Loss: 0.02971433848142624\n",
      "Epoch 73/100, Loss: 0.029285738244652748\n",
      "Epoch 74/100, Loss: 0.028865864500403404\n",
      "Epoch 75/100, Loss: 0.028466997668147087\n",
      "Epoch 76/100, Loss: 0.028086142614483833\n",
      "Epoch 77/100, Loss: 0.027716848999261856\n",
      "Epoch 78/100, Loss: 0.027366703376173973\n",
      "Epoch 79/100, Loss: 0.02702898345887661\n",
      "Epoch 80/100, Loss: 0.026698898524045944\n",
      "Epoch 81/100, Loss: 0.02638367749750614\n",
      "Epoch 82/100, Loss: 0.026077741757035255\n",
      "Epoch 83/100, Loss: 0.025781884789466858\n",
      "Epoch 84/100, Loss: 0.02549567259848118\n",
      "Epoch 85/100, Loss: 0.025400586426258087\n",
      "Epoch 86/100, Loss: 0.02513064071536064\n",
      "Epoch 87/100, Loss: 0.024872738867998123\n",
      "Epoch 88/100, Loss: 0.024622537195682526\n",
      "Epoch 89/100, Loss: 0.024382218718528748\n",
      "Epoch 90/100, Loss: 0.024149037897586823\n",
      "Epoch 91/100, Loss: 0.02392170950770378\n",
      "Epoch 92/100, Loss: 0.0237016212195158\n",
      "Epoch 93/100, Loss: 0.02348688803613186\n",
      "Epoch 94/100, Loss: 0.023279475048184395\n",
      "Epoch 95/100, Loss: 0.023079583421349525\n",
      "Epoch 96/100, Loss: 0.0228863637894392\n",
      "Epoch 97/100, Loss: 0.02269987016916275\n",
      "Epoch 98/100, Loss: 0.02251841500401497\n",
      "Epoch 99/100, Loss: 0.022342486307024956\n",
      "Epoch 100/100, Loss: 0.02217192016541958\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor).squeeze()            # Forward pass and remove the extra dimension\n",
    "    loss = criterion(outputs, y_train_tensor.float())    # Calculate loss\n",
    "    loss.backward()                                      # Backward pass\n",
    "    optimizer.step()                                     # Update weights\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9983\n",
      "Classification Report:\n",
      "[[56276   187]\n",
      " [    0 56839]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_probs = model(X_test_tensor).squeeze()\n",
    "    y_pred = (y_pred_probs > 0.5).float()         # Convert probabilities to binary predictions\n",
    "\n",
    "\n",
    "# Evaluate performance\n",
    "print(f'Accuracy: {accuracy_score(y_test_tensor, y_pred):.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test_tensor, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
